{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random as random\n",
    "from scipy.special import erf\n",
    "from math import sqrt,pi\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import email \n",
    "import os, sys, stat\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.svm import LinearSVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Email Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dir_train = 'CSDMC2010_SPAM/TRAINING'\n",
    "stop = set(stopwords.words('english'))\n",
    "sno = SnowballStemmer('english')\n",
    "\n",
    "train_data = []\n",
    "# k = 0\n",
    "for name in sorted(os.listdir(Dir_train)):\n",
    "#     if k>= 15000:\n",
    "\n",
    "#         break\n",
    "        \n",
    "    file_data = ''\n",
    "    file = Dir_train + '/' + name\n",
    "    f = open(file,encoding=\"Latin-1\",mode = 'r')\n",
    "    data = f.read()\n",
    "    \n",
    "    ###### Reading and segmenting out email body and subject\n",
    "    msg = email.message_from_string(data)\n",
    "    payload = msg.get_payload()\n",
    "    if type(payload) == type(list()) :\n",
    "        payload = payload[0]\n",
    "    sub = msg.get('subject')\n",
    "    sub = str(sub)\n",
    "    if type(payload) != type('') :\n",
    "        payload = str(payload)\n",
    "    \n",
    "    ###### TOkenizing and preprocessing the the email contents    \n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    cleaned = TAG_RE.sub('', sub + ' ' + payload)\n",
    "    cleaned = word_tokenize(cleaned)\n",
    "    filt = [words for words in cleaned if words not in stop]\n",
    "    filt = [words.lower() for words in filt if words.isalpha() and len(words)>2]\n",
    "    filt = [sno.stem(words) for words in filt]\n",
    "    \n",
    "    ###### Preparing input for the count tokenizer\n",
    "    for words in filt:\n",
    "        file_data += ' ' + words\n",
    "        \n",
    "    train_data.append(file_data)\n",
    "#     k += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming label vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_file = 'CSDMC2010_SPAM/SPAMTrain.label'\n",
    "label= np.genfromtxt(label_file,delimiter = ' ')[:,0]\n",
    "y_label = np.zeros((len(label)))\n",
    "for i in range(len(label)):\n",
    "    if label[i] == 1.0:\n",
    "        y_label[i] = 1\n",
    "    else:\n",
    "        y_label[i] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = text.TfidfVectorizer()\n",
    "x_train = vectorizer.fit_transform(train_data)\n",
    "vocab = vectorizer.vocabulary_\n",
    "x_train=x_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(binary=True,max_features=1000,lowercase=True)\n",
    "x_train = vectorizer.fit_transform(train_data)\n",
    "vocab = vectorizer.vocabulary_\n",
    "x_train=x_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(4327, 1000)\n",
      "(4327,)\n",
      "(4327, 1000)\n",
      "1000\n",
      "(1000, 1000)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import random as rd\n",
    "print(x_train.shape[1])\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_label.shape)\n",
    "y_label=y_label.reshape(-1,1)\n",
    "X=np.concatenate((x_train,y_label),axis=1);\n",
    "print(x_train.shape)\n",
    "\n",
    "np.random.seed(2)\n",
    "np.random.shuffle(X)\n",
    "\n",
    "x_train=X[:,0:-1];\n",
    "y_label=X[:,-1]\n",
    "\n",
    "samples=1000;\n",
    "xtrain=np.zeros((samples,x_train.shape[1]));\n",
    "xtest=np.zeros((samples,x_train.shape[1]));\n",
    "\n",
    "\n",
    "ind1=np.argwhere(y_label==1);\n",
    "xtrain[0:int(samples/2),:]=x_train[ind1[0:int(samples/2),0],:]\n",
    "xtest[0:int(samples/2),:]=x_train[ind1[int(samples/2):samples,0],:]\n",
    "\n",
    "ind2=np.argwhere(y_label==-1);\n",
    "xtrain[int(samples/2):samples,:]=x_train[ind2[0:int(samples/2),0],:]\n",
    "xtest[int(samples/2):samples,:]=x_train[ind2[int(samples/2):samples,0],:]\n",
    "\n",
    "ytrain=np.ones(samples)\n",
    "ytrain[int(samples/2):samples]=-1\n",
    "\n",
    "ytest=np.ones(samples)\n",
    "ytest[int(samples/2):samples]=-1\n",
    "\n",
    "\n",
    "# Perturbation vs Fscore\n",
    "# clf = LinearSVC(max_iter=1000,dual=False, penalty='l1')\n",
    "# clf.fit(xtrain,ytrain)\n",
    "\n",
    "\n",
    "# d=[0,10,20,30,40,50,100]\n",
    "# f1_svm=np.zeros(len(d));\n",
    "# f1_nash=np.zeros(len(d));\n",
    "# k=0;\n",
    "\n",
    "# np.random.seed(2000)\n",
    "# w1=np.random.multivariate_normal(mean, cov,1)\n",
    "# w1=w1[0,:]\n",
    "\n",
    "\n",
    "\n",
    "# for dmax in d:\n",
    "# #Perturbing the test data\n",
    "#     X=xtest[int(samples/2):samples,:]\n",
    "\n",
    "#     for i in range(X.shape[0]):\n",
    "#         rd.seed(i)\n",
    "#         ind=rd.sample(range(1000),dmax)\n",
    "#         for j in range(dmax):\n",
    "#             if (X[i,ind[j]]==0):\n",
    "#                 X[i,ind[j]]=1;\n",
    "#             elif (X[i,ind[j]]==1):\n",
    "#                 X[i,ind[j]]=0;\n",
    "#     xtest[int(samples/2):samples,:]=X;\n",
    "#     ypred1 = clf.predict(xtest);\n",
    "#     f1_svm[k]=f1_score(ytest,ypred1);\n",
    "#     ypred2=predict(w1,xtest)\n",
    "#     f1_nash[k]=f1_score(ytest,ypred2);\n",
    "#     k=k+1;\n",
    "\n",
    "# plt.plot(d,f1_svm,label='SVM')\n",
    "# plt.plot(d,f1_nash,label='RNash SVM')\n",
    "# # plt.title('F1 Score vs d',fontsize=20)\n",
    "# plt.xlabel('d',fontsize=16)\n",
    "# plt.ylabel('F1 Score',fontsize=16)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "#-----------------------------------------------\n",
    "# PCA \n",
    "#-----------------------------------------------\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(xtrain)\n",
    "# xtrain=pca.transform(xtrain)\n",
    "# xtest=pca.transform(xtest)\n",
    "\n",
    "#Perturbing the test data\n",
    "X=xtest[int(samples/2):samples,:]\n",
    "dmax=100\n",
    "for i in range(X.shape[0]):\n",
    "    rd.seed(i)\n",
    "    ind=rd.sample(range(1000),dmax)\n",
    "    for j in range(dmax):\n",
    "        if (X[i,ind[j]]==0):\n",
    "            X[i,ind[j]]=1;\n",
    "        elif (X[i,ind[j]]==1):\n",
    "            X[i,ind[j]]=0;\n",
    "xtest[int(samples/2):samples,:]=X;\n",
    "\n",
    "# Plot of data\n",
    "def get_plot(w,w1,w2,xtrain):\n",
    "    xmin=xtrain[:,0].min()\n",
    "    xmax=xtrain[:,0].max()\n",
    "    xx=np.linspace(xmin,xmax,num=200)\n",
    "    yy=(-(w[0]/w[1])*xx)#-(w[2]/w[1])\n",
    "    yy1=(-(w1[0]/w1[1])*xx)#-(w[2]/w[1])\n",
    "    yy2=(-(w2[0]/w2[1])*xx)#-(w[2]/w[1])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     yy=(-(w[0]/w[1])*xx)-(w[2]/w[1])\n",
    "#     yy1=(-(w1[0]/w1[1])*xx)-(w[2]/w[1])\n",
    "#     yy2=(-(w2[0]/w2[1])*xx)-(w[2]/w[1])\n",
    "    samples=xtrain.shape[0];\n",
    "    \n",
    "    plt.plot(xtrain[0:int(samples/2),0],xtrain[0:int(samples/2),1],'bo',label='Class 1')\n",
    "    plt.plot(xtrain[int(samples/2):samples,0],xtrain[int(samples/2):samples,1],'yo',label='Class -1')\n",
    "#     plt.plot(xmean[0,:],xmean[1,:],'ro',label='Mean')\n",
    "    plt.plot(xx,yy,'k-')\n",
    "    plt.plot(xx,yy1,'r-')\n",
    "    plt.plot(xx,yy2,'r-')\n",
    "    plt.xlabel('Feature-1')\n",
    "    plt.ylabel('Feature-2')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# xtrain=np.c_[xtrain,np.ones((len(ytrain),1))];\n",
    "# xtest=np.c_[xtest,np.ones((len(ytest),1))];\n",
    "\n",
    "print(xtrain.shape[0])\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual +1=  500  classified as +1=  472\n",
      "Actual -1=  500  classified as -1=  354\n",
      "Accuracy=  77.5\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(ypred,ytest):\n",
    "    acc=0;\n",
    "    for i in range(len(ypred)):\n",
    "        if(ypred[i]==ytest[i]):\n",
    "            acc+=1;\n",
    "    acc=acc/len(ypred);\n",
    "    return acc*100\n",
    "\n",
    "def get_class_accuracy(ypred,ytest):\n",
    "    acc_1=0;\n",
    "    acc_0=0;\n",
    "    c1=0;\n",
    "    c0=0;\n",
    "    for i in range(len(ypred)):\n",
    "        if(ytest[i]==1):\n",
    "            c1+=1;\n",
    "            if(ypred[i]==ytest[i]):\n",
    "                acc_1+=1;\n",
    "        if(ytest[i]==-1):\n",
    "            c0+=1;\n",
    "            if(ypred[i]==ytest[i]):\n",
    "                acc_0+=1;       \n",
    "    return acc_1,acc_0,c1,c0\n",
    "\n",
    "def predict(W,xtest):\n",
    "    def sign(x):\n",
    "        if x>0:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    n,d=xtest.shape;\n",
    "    y_out=np.zeros(n);\n",
    "    for i in range(n):\n",
    "        xi=xtest[i,:];\n",
    "        y_out[i]=sign(W@xi);\n",
    "    return y_out;\n",
    "\n",
    "clf = LinearSVC(max_iter=1000,dual=False, penalty='l1')\n",
    "clf.fit(xtrain,ytrain)\n",
    "ypred = clf.predict(xtest)\n",
    "cl1,cl0,ac1,ac0=get_class_accuracy(ypred,ytest)\n",
    "print('Actual +1= ',ac1,' classified as +1= ',cl1)\n",
    "print('Actual -1= ',ac0,' classified as -1= ',cl0)\n",
    "print('Accuracy= ',acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Randomized Prediction Game Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "# Gradient Computation\n",
    "#---------------------------------------------------------------------------------------------\n",
    "def get_gradient(thetaL,thetaD,xtrain,ytrain,pl,pd):\n",
    "    rl=1\n",
    "    rd=pl/pd;\n",
    "    n,m=xtrain.shape;\n",
    "    \n",
    "    mu_w=thetaL[:,0];\n",
    "    mu_w_bar=thetaL[0:m-1,0];\n",
    "    mu_b=thetaL[m-1,0];\n",
    "\n",
    "    sigma_w=thetaL[:,1];\n",
    "    sigma_w_bar=thetaL[0:m-1,1];\n",
    "    sigma_b=thetaL[m-1,1];\n",
    "\n",
    "    mu_x=thetaD[:,0:n];\n",
    "    sigma_x=thetaD[:,n:2*n];\n",
    "\n",
    "\n",
    "    # Gradient Computation of Learner's cost wrt learner's parameter(mean & variance)\n",
    "    grad_l=np.zeros((m,2));\n",
    "    mu_si=1-np.multiply(ytrain,((mu_w_bar.T@mu_x)+mu_b));\n",
    "    sigma_si_2=np.square(sigma_w_bar).T@(np.square(mu_x)+np.square(sigma_x))+\\\n",
    "                np.square(mu_w_bar).T@np.square(sigma_x)+sigma_b**2;\n",
    "        \n",
    "    # Gradient of learner's cost function wrt Mean of W\n",
    "    del_h_by_del_mu_si           = 0.5*(1-erf((-1/sqrt(2))*np.divide(mu_si,np.sqrt(sigma_si_2))));\n",
    "    del_mu_si_by_del_mu_w        = -np.append(mu_x,np.ones((1,n)),axis=0)*ytrain;\n",
    "    \n",
    "    del_h_by_del_sigma_si2       = (1/2*sqrt(2*pi))*np.divide(np.exp((-0.5)*\\\n",
    "                                   np.divide(np.square(mu_si),sigma_si_2)),np.sqrt(sigma_si_2));\n",
    "    \n",
    "    del_sigma_si2_by_del_mu_w    = np.append((2*np.square(sigma_x))*mu_w_bar[:,np.newaxis],np.zeros((1,n)),axis=0)      \n",
    "\n",
    "    del_Ll_by_del_mu_w           = pl*np.append(mu_w_bar,0)+np.sum((del_mu_si_by_del_mu_w*del_h_by_del_mu_si),axis=1)+\\\n",
    "                                   np.sum((del_sigma_si2_by_del_mu_w*del_h_by_del_sigma_si2),axis=1)\n",
    "\n",
    "    # Gradient of learner's cost function wrt variance of W\n",
    "    del_sigma_si2_by_del_sigma_w =2*((np.append(np.square(sigma_x)+np.square(mu_x),\\\n",
    "                                            np.ones((1,n)),axis=0))*sigma_w[:,np.newaxis]);\n",
    "\n",
    "\n",
    "    del_Ll_by_del_sigma_w        = pl*np.append(sigma_w_bar,0)+np.sum((del_sigma_si2_by_del_sigma_w*\\\n",
    "                                   del_h_by_del_sigma_si2),axis=1)   \n",
    "\n",
    "\n",
    "    grad_l[:,0]=del_Ll_by_del_mu_w ;\n",
    "    grad_l[:,1]=del_Ll_by_del_sigma_w;\n",
    "    # Gradient Computation of Data Generator's cost wrt its parameters\n",
    "    grad_dg=np.zeros((m-1,2*n));\n",
    "    \n",
    "    for i in range(n):\n",
    "        xi_hat=xtrain[i,0:m-1];\n",
    "        mu_xi=mu_x[:,i];\n",
    "        sigma_xi=sigma_x[:,i];\n",
    "\n",
    "        mu_ti                       = 1+np.multiply(ytrain[i],((mu_w_bar.T@mu_xi)+mu_b));\n",
    "        sigma_ti_2                  = np.square(sigma_w_bar).T@(np.square(mu_xi)+np.square(sigma_xi))+\\\n",
    "                                      np.square(mu_w_bar).T@np.square(sigma_xi)+sigma_b**2;\n",
    "\n",
    "        del_h_by_del_mu_ti          = 0.5*(1-erf((-1/sqrt(2))*np.divide(mu_ti,np.sqrt(sigma_ti_2))));\n",
    "        del_h_by_del_sigma_ti2      = (1/2*sqrt(2*pi))*np.divide(np.exp((-0.5)*\\\n",
    "                                       np.divide(np.square(mu_ti),sigma_ti_2)),np.sqrt(sigma_ti_2));\n",
    "\n",
    "        del_mu_ti_by_del_mu_xi      = ytrain[i]*mu_w_bar;\n",
    "        del_sigma_ti2_del_mu_xi     = np.multiply(2*np.square(sigma_w_bar),mu_xi)\n",
    "\n",
    "        del_sigma_ti2_del_sigma_xi  = np.multiply(2*sigma_xi,(np.square(sigma_w_bar)+np.square(mu_w_bar)))\n",
    "\n",
    "        del_Ld_by_del_mu_xi         = (mu_xi-xi_hat)+pd*((del_h_by_del_mu_ti*del_mu_ti_by_del_mu_xi)+\\\n",
    "                                      (del_h_by_del_sigma_ti2*del_sigma_ti2_del_mu_xi))\n",
    "\n",
    "        del_Ld_by_del_sigma_xi      = sigma_xi+pd*(del_h_by_del_sigma_ti2*del_sigma_ti2_del_sigma_xi)\n",
    "        \n",
    "        grad_dg[:,i]   = del_Ld_by_del_mu_xi;\n",
    "        grad_dg[:,n+i] = del_Ld_by_del_sigma_xi;\n",
    "        grad_dg=rd*grad_dg;\n",
    "        # grad_l is the gradientof learner's cost wrt thetaL. It will have same no. of element as thetaD \n",
    "        # grad_dg is the gradient of data generator's cost wrt thetaD.\n",
    "    return grad_l,grad_dg;\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Bound\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def bound(temp_l,temp_dg): \n",
    "    # Initializing the upper and lower bound for all the parameters\n",
    "    # Learner's mean\n",
    "    W=0.01;\n",
    "    mu_l_max=W;\n",
    "    mu_l_min=-W;\n",
    "    # Data Generator's mean\n",
    "    mu_dg_max=1;\n",
    "    mu_dg_min=0;\n",
    "    # Learner's variance\n",
    "    var_l_max=10**(-3);\n",
    "    sigma_l_max=sqrt(var_l_max);\n",
    "    var_l_min=10**(-6);\n",
    "    sigma_l_min=sqrt(var_l_min);\n",
    "    # Data Generator's Variance\n",
    "    var_dg_max=0.5;\n",
    "    sigma_dg_max=sqrt(var_dg_max)\n",
    "    var_dg_min=10**(-3);\n",
    "    sigma_dg_min=sqrt(var_dg_min)\n",
    "\n",
    "    # Bounding mean of learner \n",
    "    ind=np.where(temp_l[:,0]>mu_l_max)\n",
    "    temp_l[ind,0]=mu_l_max;\n",
    "    ind=np.argwhere(temp_l[:,0]<mu_l_min)\n",
    "    temp_l[ind,0]=mu_l_min;\n",
    "\n",
    "    # Bounding std dev of learner\n",
    "    ind=np.where(temp_l[:,1]>sigma_l_max)\n",
    "    temp_l[ind,1]=sigma_l_max;\n",
    "    ind=np.where(temp_l[:,1]<sigma_l_min)\n",
    "    temp_l[ind,1]=sigma_l_min;\n",
    "\n",
    "    x=temp_dg.shape\n",
    "    n=int(x[1]/2);\n",
    "\n",
    "    # # Bounding mean of data generator\n",
    "    for i in range(n):\n",
    "        ind=np.where(temp_dg[:,i]>mu_dg_max)\n",
    "        temp_dg[ind,i]=mu_dg_max;\n",
    "        ind=np.where(temp_dg[:,i]<mu_dg_min)\n",
    "        temp_dg[ind,i]=mu_dg_min\n",
    "    # # Bounding std dev of data generator\n",
    "    for i in range(n,2*n):\n",
    "        ind=np.where(temp_dg[:,i]>sigma_dg_max)\n",
    "        temp_dg[ind,i]=sigma_dg_max;\n",
    "        ind=np.where(temp_dg[:,i]<sigma_dg_min)\n",
    "        temp_dg[ind,i]=sigma_dg_min\n",
    "    return temp_l,temp_dg;\n",
    "\n",
    "def get_norm(d_l,d_dg):\n",
    "    norm1=np.square(d_l).sum()+np.square(d_dg).sum()\n",
    "    return norm1\n",
    "\n",
    "def get_dot(grad_l,grad_dg,d_l,d_dg):\n",
    "    temp=np.multiply(grad_l,d_l).sum()+np.multiply(grad_dg,d_dg).sum();\n",
    "    return temp\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "# Extra Gradient\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "def extraGradient(xtrain,ytrain,sigma,beta,pl,pd,eps):\n",
    "    n,m=xtrain.shape;\n",
    "    # Initializing stategy sets of both the players. For learner strategy is parameters of distribution of W\n",
    "    # Parameters include mean and standard deviation of multi dimension gaussian from which W is sampled.\n",
    "    # Parameters are stored in two columns. First column corresponds to mean vector(mu_w) of m dimension and \n",
    "    # second column is std dev vector(sigma_w)\n",
    "    thetaL=random.rand(m,2);# concatenation of mu_w and sigma_w\n",
    "    # Strategy set of data generator consist of distribution parameters for each sample in the training set. For each sample\n",
    "    # We have one mean vector and one std dev vector. For n samples we will have 2n vectors arranged in columns of thetaD\n",
    "    # First n columns corresponds to mean vector of n samples and last n columns corresponds to std dev vector of n samples\n",
    "    thetaD=random.rand(m-1,2*n);# n columns for mu and n columns for sigma\n",
    "\n",
    "    thetaL,thetaD=bound(thetaL,thetaD)\n",
    "    \n",
    "    \n",
    "    k=0;\n",
    "\n",
    "    flag=1;\n",
    "    #Extra Gradient Algorithm\n",
    "    while(flag==1):\n",
    "        # Step 5\n",
    "#         get_plot(thetaL[:,0],thetaL[:,0]-thetaL[:,1],thetaL[:,0]+thetaL[:,1],xtrain)\n",
    "#         print(thetaL)\n",
    "        grad_l,grad_dg=get_gradient(thetaL,thetaD,xtrain,ytrain,pl,pd);\n",
    "\n",
    "        temp_l=thetaL-grad_l;\n",
    "        temp_dg=thetaD-grad_dg;\n",
    "\n",
    "        l_bounded,dg_bounded=bound(temp_l,temp_dg)\n",
    "\n",
    "        d_l=l_bounded-thetaL;\n",
    "        d_dg=dg_bounded-thetaD;\n",
    "        \n",
    "#         ## Print norm of d\n",
    "        d_norm=get_norm(d_l,d_dg)\n",
    "#         print('Norm of d= ',d_norm)\n",
    "        # Step 6\n",
    "        flag1=1\n",
    "        p=1\n",
    "        while (flag1==1):\n",
    "            print('p=',p)\n",
    "            t=beta**p;\n",
    "            p+=1;\n",
    "            \n",
    "            thetaL_bar=thetaL+t*d_l;\n",
    "            thetaD_bar=thetaD+t*d_dg;\n",
    "\n",
    "            grad_l_bar,grad_dg_bar=get_gradient(thetaL_bar,thetaD_bar,xtrain,ytrain,pl,pd);\n",
    "            d_norm=get_norm(d_l,d_dg);\n",
    "            g_dot_d=get_dot(grad_l_bar,grad_dg_bar,d_l,d_dg);\n",
    "            if ((-g_dot_d) >= (sigma*d_norm)):\n",
    "                flag1=0\n",
    "\n",
    "#         print('Norm of theta_bar-theta= ',get_norm(thetaL_bar-thetaL,thetaD_bar-thetaD))\n",
    "        # Step 7\n",
    "#         thetaL_bar=thetaL+t*d_l;\n",
    "#         thetaD_bar=thetaD+t*d_dg;\n",
    "#         grad_l_bar,grad_dg_bar=get_gradient(thetaL_bar,thetaD_bar,xtrain,ytrain,pl,pd);\n",
    "\n",
    "        eta=(-t/get_norm(grad_l_bar,grad_dg_bar))*get_dot(grad_l_bar,grad_dg_bar,d_l,d_dg)\n",
    "#         print(grad_l_bar)\n",
    "        # Step 8\n",
    "        temp_l=thetaL-eta*grad_l_bar;\n",
    "        temp_dg=thetaD-eta*grad_dg_bar;\n",
    "\n",
    "        new_thetaL,new_thetaD=bound(temp_l,temp_dg)\n",
    "\n",
    "        # Step 9\n",
    "        k=k+1\n",
    "        print('iteration= ',k)\n",
    "\n",
    "        # Step 10\n",
    "        error=get_norm((new_thetaL-thetaL),(new_thetaD-thetaD))\n",
    "        print('error=',error)\n",
    "        thetaL=np.copy(new_thetaL);\n",
    "        thetaD=np.copy(new_thetaD);\n",
    "\n",
    "        if (error<eps):\n",
    "            flag=0;\n",
    "        if (k>50):\n",
    "            flag=0;\n",
    "    return thetaL    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 1\n",
      "iteration=  1\n",
      "error= 23883.4018318\n",
      "p= 1\n",
      "iteration=  2\n",
      "error= 23.1685411556\n",
      "p= 1\n",
      "iteration=  3\n",
      "error= 5335.57670596\n",
      "p= 1\n",
      "iteration=  4\n",
      "error= 3038.68335894\n",
      "p= 1\n",
      "iteration=  5\n",
      "error= 1550.77954006\n",
      "p= 1\n",
      "iteration=  6\n",
      "error= 908.938810372\n",
      "p= 1\n",
      "iteration=  7\n",
      "error= 470.32309666\n",
      "p= 1\n",
      "iteration=  8\n",
      "error= 332.178540467\n",
      "p= 1\n",
      "iteration=  9\n",
      "error= 272.729352596\n",
      "p= 1\n",
      "iteration=  10\n",
      "error= 176.839285638\n",
      "p= 1\n",
      "iteration=  11\n",
      "error= 153.998925944\n",
      "p= 1\n",
      "iteration=  12\n",
      "error= 108.445737901\n",
      "p= 1\n",
      "iteration=  13\n",
      "error= 102.377051548\n",
      "p= 1\n",
      "iteration=  14\n",
      "error= 83.612093814\n",
      "p= 1\n",
      "iteration=  15\n",
      "error= 70.3123584492\n",
      "p= 1\n",
      "iteration=  16\n",
      "error= 57.1877879971\n",
      "p= 1\n",
      "iteration=  17\n",
      "error= 49.3063904236\n",
      "p= 1\n",
      "iteration=  18\n",
      "error= 41.3943228211\n",
      "p= 1\n",
      "iteration=  19\n",
      "error= 37.7855983482\n",
      "p= 1\n",
      "iteration=  20\n",
      "error= 32.6381535965\n",
      "p= 1\n",
      "iteration=  21\n",
      "error= 28.6690877867\n",
      "p= 1\n",
      "iteration=  22\n",
      "error= 25.0716526418\n",
      "p= 1\n",
      "iteration=  23\n",
      "error= 22.2924033461\n",
      "p= 1\n",
      "iteration=  24\n",
      "error= 19.683778754\n",
      "p= 1\n",
      "iteration=  25\n",
      "error= 17.7007662569\n",
      "p= 1\n",
      "iteration=  26\n",
      "error= 15.731134394\n",
      "p= 1\n",
      "iteration=  27\n",
      "error= 14.3062057577\n",
      "p= 1\n",
      "iteration=  28\n",
      "error= 12.6245209799\n",
      "p= 1\n",
      "iteration=  29\n",
      "error= 11.7136442197\n",
      "p= 1\n",
      "iteration=  30\n",
      "error= 10.4147472507\n",
      "p= 1\n",
      "iteration=  31\n",
      "error= 9.73563310677\n",
      "p= 1\n",
      "iteration=  32\n",
      "error= 8.82848994708\n",
      "p= 1\n",
      "iteration=  33\n",
      "error= 8.20563636009\n",
      "p= 1\n",
      "iteration=  34\n",
      "error= 7.50697113269\n",
      "p= 1\n",
      "iteration=  35\n",
      "error= 6.97747869328\n",
      "p= 1\n",
      "iteration=  36\n",
      "error= 6.40862644405\n",
      "p= 1\n",
      "iteration=  37\n",
      "error= 5.9834087626\n",
      "p= 1\n",
      "iteration=  38\n",
      "error= 5.51737586787\n",
      "p= 1\n",
      "iteration=  39\n",
      "error= 5.17217965615\n",
      "p= 1\n",
      "iteration=  40\n",
      "error= 4.78884528868\n",
      "p= 1\n",
      "iteration=  41\n",
      "error= 4.50398206154\n",
      "p= 1\n",
      "iteration=  42\n",
      "error= 4.1838033351\n",
      "p= 1\n",
      "iteration=  43\n",
      "error= 3.9479249242\n",
      "p= 1\n",
      "iteration=  44\n",
      "error= 3.67952346211\n",
      "p= 1\n",
      "iteration=  45\n",
      "error= 3.48285902923\n",
      "p= 1\n",
      "iteration=  46\n",
      "error= 3.25363678648\n",
      "p= 1\n",
      "iteration=  47\n",
      "error= 3.08858948335\n",
      "p= 1\n",
      "iteration=  48\n",
      "error= 2.89346516622\n",
      "p= 1\n",
      "iteration=  49\n",
      "error= 2.75345349544\n",
      "p= 1\n",
      "iteration=  50\n",
      "error= 2.58499187543\n",
      "p= 1\n",
      "iteration=  51\n",
      "error= 2.46602653768\n"
     ]
    }
   ],
   "source": [
    "# Rows of xtrain corresponds to samples and columns corresponds to features\n",
    "sigma=0.01\n",
    "beta=0.5\n",
    "pl=1\n",
    "pd=1\n",
    "eps=0.001;# Stopping threshold\n",
    "w=extraGradient(xtrain,ytrain,sigma,beta,pl,pd,eps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual +1=  500  classified as +1=  371\n",
      "Actual -1=  500  classified as -1=  404\n",
      "Accuracy=  77.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Randomly selecting one sample from normal distribution\n",
    "mean=w[:,0]\n",
    "cov=np.diagflat(w[:,1])\n",
    "np.random.seed(2000)\n",
    "# np.random.seed(2000)# perturbed data\n",
    "w1=np.random.multivariate_normal(mean, cov,1)\n",
    "w1=w1[0,:]\n",
    "\n",
    "# ypred=predict(w1[:,0],xtest)\n",
    "ypred=predict(w1,xtest)\n",
    "# print(ypred)\n",
    "acc=get_accuracy(ypred,ytest)\n",
    "cl1,cl0,ac1,ac0=get_class_accuracy(ypred,ytest)\n",
    "print('Actual +1= ',ac1,' classified as +1= ',cl1)\n",
    "print('Actual -1= ',ac0,' classified as -1= ',cl0)\n",
    "print('Accuracy= ',acc)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
